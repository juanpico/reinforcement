{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Universidad de los Andes<br>Departamento de Ingeniería Eléctrica y Electrónica<br>IELE 4922 - Reinforcement Learning<br><br><p style=\"font-size:30px\">Taller 2 - Parte II: Value y Policy Iteration </p> \t|[<img src=\"images/uniandes_logo.png\" width=\"250\"/>](images/uniandes_logo.png)  \t|\n",
    "|-----------------------------------------------------------------------------------\t|---\t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autores\n",
    "- Juan Camilo Pico Garrido - 201712801\n",
    "- Mauricio Ricardo Delgado Quintero - 201712801"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este es el cuaderno *(Jupyter Notebook)* base para desarrollar la segunda parte del taller 2 del curso de *Reinforcement Learning (2021-10)*. En este se busca implementar los métodos de *Value Iteration* y *Policy Iteration* en un *gridworld*. Con este entorno se puede visualizar de forma gráfica e interactiva el comportamiento de estos métodos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# El entorno: *Gridworld*\n",
    "El entorno consiste en una cuadrícula de 3x4, donde cada celda corresponde a una posible ubicación del agente o estado. El agente en cada celda tiene cuatro acciones posibles de movimiento: norte, sur, este y oeste (N, S, E, O). Sin embargo, estas acciones NO son confiables. Con una probabilidad de 0.8 el agente se moverá en la dirección prevista, y con una probabilidad de 0.2, el agente se moverá en una dirección aleatoria contigua. Es decir si se quiere mover al norte (N), las direcciones contiguas a las que se podrá mover con una probabilidad de 0.2 serán al este (E) o al oeste (O). Adicionalmente, aquellas acciones que lleven al agente fuera de la cuadrícula o a posiciones bloqueadas, no tendrán efecto en la posición del agente.\n",
    "\n",
    "El estado inicial del agente será en la celda (2,0). Si el agente llega a la celda con el diamante, recibirá una recompensa de +1.0 y el juego (*episodio*) terminará. Si el agente llega a la celda con la bomba, recibirá una recompensa de -1.0 y el episodio terminará. Por ende, los estados (0,3) y (1,3) serán considerados *estados terminales*. El agente recibe recompensa 0.0 por llegar a cualquier otra celda. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"images/gridworld.png\" alt=\"centered image\" width=300/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consideraciones importantes:**\n",
    "\n",
    "* Para la correcta ejecución del código dado mantenga los nombres de las variables propuestos.\n",
    "\n",
    "* Debe editar y completar unicamente las celdas que comiencen con la instrucción #EDITABLE\n",
    "\n",
    "* No se preocupe si la ventana de gridworld no responde en algunos momentos, una vez ejecute los métodos para la evaluación (Todos aquellos deppues de las instrucciones para la **visualización**) esta se actualizará y podrá ver de forma interactiva la actualización de los valores deseados.\n",
    "\n",
    "* Si quiere volver a visualizar el gridworld, debe volver a ejecutar al menos todas las celdas de esa sección."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.18, Python 3.8.8)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "# Librerias que contienen la dinámica del entorno gridworld\n",
    "from types import MethodType\n",
    "from gridworld import GridWorld, pygame\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea el objeto *gridworld* con las siguientes característica:\n",
    "* Tamaño de la cuadrícula: 3 filas y 4 columnas\n",
    "* La celda bloqueda estará en (1,1)\n",
    "* La celda con la bomba estará en (1,3)\n",
    "* La celda con el diamante estará en (0,3)\n",
    "\n",
    "Una vez creado el objeto, en una ventana emergente de pygame se encuentra la visualización del *gridworld*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw = GridWorld(rows=3, cols=4, walls=[(1,1)], pits=[(1,3)], goals=[(0,3)], live_reward=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Espacio de estados\n",
    "Para acceder a una lista con los estados del gridworld se puede utilizar la propiedad *states* del objeto gridworld, así: `gw.states`\n",
    "\n",
    "¿Cuántos estados tiene el problema?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EDITABLE\n",
    "len(gw.states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado esperado:**\n",
    "\n",
    "```\n",
    "num_states = 11\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Espacio de acciones\n",
    "\n",
    "Para explorar el espacio de acciones puede hacer uso de la función `gw.get_allowed_actions(state)`, que recibe como parámetro el estado. Este estado se define como una tupla (*x,y*), con la posición donde se encuentra el agente.\n",
    "\n",
    "\n",
    "¿Cuál es el espacio de acciones en el estado inicial?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['N', 'S', 'E', 'W']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EDITABLE\n",
    "gw.get_allowed_actions((0,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado esperado:**\n",
    "```\n",
    "allowed_actions = ['N', 'S', 'E', 'W']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No obstante, se sabe que las acciones no son confiables ya que tienen un ruido inherente. Esto es, con una probabilidad de 0.8 el agente se mueve en la dirección prevista, y con una probabilidad de 0.2, el agente se mueve en una dirección aleatoria hacia los lados.\n",
    "\n",
    "Por ejemplo, si la acción deseada es 'N', con probabilidad de 0.8 se moverá al norte. Con probabilidad de 0.1 al este y con probabilidad de 0.1 al oeste.\n",
    "\n",
    "Verifique esto para la acción 'E'. Utilice los atributos `gw.real_actions[action]` y `gw.action_probabilitites`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E', 'N', 'S']\n",
      "[0.8, 0.1, 0.1]\n"
     ]
    }
   ],
   "source": [
    "# EDITABLE\n",
    "print(gw.real_actions['E'])\n",
    "print(gw.action_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado esperado:**\n",
    "```\n",
    "['E', 'N', 'S']\n",
    "[0.8, 0.1, 0.1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicación de la acción\n",
    "Una vez se conoce el estado actual y la acción que va a ejecutar el agente, esta se puede aplicar al entorno. En respuesta, el agente percibe el nuevo estado del entorno, así como una señal de recompensa que indica que tan buena fue se acción. Igualmente, se recibe una señal de *done* que indica si se alcanzó un estado terminal. En este caso, el episodio finaliza.\n",
    "\n",
    "<center>\n",
    "    <img src=\"images/rl_bloques.PNG\" alt=\"centered image\" width=350/>\n",
    "</center>\n",
    "\n",
    "La recompensa del agente del gridworld se define como:\n",
    "\\begin{equation*}\n",
    "R(s,a,s') = \\begin{cases}\n",
    "1 &\\text{si $s=(0,3)$}\\\\\n",
    "-1 &\\text{si $s=(1,3)$}\\\\\n",
    "0 &\\text{d.l.c}\n",
    "\\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "El método `gw.step(state, action, random)` permite aplicar una acción al gridworld. Este método retorna el nuevo estado, la recompensa, la accion ejecutada y la bandera de terminado (done), todo en ese orden. \n",
    "\n",
    "¿Qué sucede si el agente está en el estado (2,1) y realiza la acción 'N' ? Imprima los resultados con el parámetro `random = False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 0), 0.0, 'W', False)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EDITABLE\n",
    "gw.step((2,1), 'W', random = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado esperado**\n",
    "```\n",
    "new_state = (2, 1)\n",
    "reward = 0.0\n",
    "action = N\n",
    "done = False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que el agente, al tratar de moverse en la dirección bloqueada, se queda en su misma posición.\n",
    "\n",
    "Ahora bien, repita esto con el parámetro `random = True`. En este caso, dada la aleatoriedad del entorno, puede que el resultado obtenido no sea igual al anterior. No obstante, la mayoría de las veces (el 80% para ser exacto) sí lo será. Sientase en la libertad de ejecutar varias veces la celda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 1), 0.0, 'N', False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EDITABLE\n",
    "gw.step((2,1), 'N', random = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, se reinicia el entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.quit\n",
    "gw = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cierre la ventana emergente y reinicie el kernel de ejecución."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función de Valor, $V$\n",
    "\n",
    "La política $\\pi$ indica, para un entorno determinístico, cuál acción debe ejecutarse en cada estado, con el objetivo de que el agente reciba la mayor utilidad. \n",
    "\n",
    "Siguiendo una política $\\pi$,  el valor de un estado $V(s)$ en el tiempo $t$ formalmente corresponde a la suma descontada de las recompensas recibidas, si el agente tiene como estado inicial $s$ y se comporta óptimamente en adelante. En palabras simples, indica cuánta utilidad  recibiría el agente si comenzara en $s$ y se comportara bien de ahí en adelante, \n",
    "\n",
    "$$V_{t}(s)=\\sum_{s'} P(s'|s,a)(R(s,a,s')+\\gamma V_{t-1}(s'))$$\n",
    "\n",
    "Donde $\\gamma$ es un factor conocido como *tasa de descuento*. Permite balancear la preferencia de tener recompensas a corto o largo plazo.\n",
    "\n",
    "Dada un política $\\pi(s)$ es posible encontrar el valor de cada estado por medio de programación dinámica, así ($\\gamma=0.9$):\n",
    "<center>\n",
    "    <img src=\"images/fvalor.png\" alt=\"centered image\" width=650/>\n",
    "</center>\n",
    "\n",
    "Se puede encontrar el valor de cada estado del gridworld a partir de una política fija. Suponga que tenemos la siguiente política determinística:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"images/gridworld2.png\" alt=\"centered image\" width=250/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.18, Python 3.8.8)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from types import MethodType\n",
    "from gridworld import GridWorld, pygame\n",
    "import copy\n",
    "\n",
    "gw = GridWorld(3, 4, [(1,1)], [(1,3)], [(0,3)], 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defina la política deterministica propuesta en la figura:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 0): 'S', (0, 1): 'E', (0, 2): 'E', (0, 3): 'E', (1, 0): 'S', (1, 2): 'N', (1, 3): 'E', (2, 0): 'E', (2, 1): 'E', (2, 2): 'N', (2, 3): 'W'}\n"
     ]
    }
   ],
   "source": [
    "# EDITABLE\n",
    "\n",
    "state = gw.states\n",
    "action = ['S', 'E', 'E', 'E', 'S', 'N', 'E', 'E', 'E', 'N', 'W']\n",
    "# --------------------------------------------\n",
    "\n",
    "for i, s in enumerate(state):\n",
    "    gw.policy[s] = action[i] \n",
    "    \n",
    "print(gw.policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado esperado**\n",
    "```\n",
    "{(0, 0): 'S', (0, 1): 'E', (0, 2): 'E', (0, 3): 'E', (1, 0): 'S', (1, 2): 'N', (1, 3): 'E', (2, 0): 'E', (2, 1): 'E', (2, 2): 'N', (2, 3): 'W'}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se implementa la función de actualización para estimar los valores de los estados, según la política $\\pi(s)$, definida anteirormente. \n",
    "\n",
    "**Tenga en cuenta que:**\n",
    "* Se va a asumir que las acciones dadas por la política son determinísticas. Esto es, se aplican al entorno con probabilidad 1\n",
    "* La regla de actualización para estados terminales es: $V_{t}(s)=\\sum_{s'} P(s'|s,\\pi(s))R(s,a,s')$\n",
    "* La regla de actualización para estados no terminales es: $V_{t}(s)=\\sum_{s'} P(s'|s,\\pi(s))(R(s,a,s')+\\gamma V_{t-1}(s'))$\n",
    "* Para obtener el valor de un estado en t-1 usamos `v(s)=gw.state_values[state] `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_values(gw, gamma):\n",
    "    # Arreglos para los valores\n",
    "    value = dict.fromkeys(gw.states , 0.0)\n",
    "    new_values = dict.fromkeys(gw.states , 0.0)\n",
    "    \n",
    "    for state in gw.states: \n",
    "        # Tomar acción de politica dada\n",
    "        action = gw.policy[state]\n",
    "        # Dar un paso\n",
    "        new_state, reward, _, done = gw.step(state, action, random=False)\n",
    "        # Actualizar valores\n",
    "        if(done):\n",
    "            # actualización para un estado terminal\n",
    "            new_values[state] = reward\n",
    "        else:\n",
    "            # actualización para un estado no terminal\n",
    "            new_values[state] = reward + gamma*gw.state_values[new_state] #borrar\n",
    "    \n",
    "    # Copiar valores\n",
    "    value = copy.deepcopy(new_values) \n",
    "    gw.state_values = copy.deepcopy(new_values) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poner a prueba la función se va a escoger una tasa de descuento de 0.9 y a realizar la estimación de los valores durante un horizonte de 15 iteraciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "H = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualización:**\n",
    "1. Ejecute la siguiente celda y haga *clic* sobre la ventana del gridworld. \n",
    "2. Podra ver como se actualizan los valores de cada estado presionando la tecla **Espacio**. \n",
    "3. Una vez completetadas las 10 iteraciones, puede cerrar el gridworld presionando la tecla **Esc**.\n",
    "4. Si tienes un error en la función 'update_values', despues de corregirlo es necesario que vuelvas a correr las 4 celdas anteriores para poder volver a lanzar la interfaz del gridworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw.update_values = MethodType(update_values, gw)\n",
    "gw.solve_dynamic_programming(gamma=gamma, horizon=H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al cabo de 10 iteraciones, el valor de los estados, siguiendo la política $\\pi(s)$ son:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V(0, 0) = 0.478\n",
      "V(0, 1) = 0.810\n",
      "V(0, 2) = 0.900\n",
      "V(0, 3) = 1.000\n",
      "V(1, 0) = 0.531\n",
      "V(1, 2) = 0.810\n",
      "V(1, 3) = -1.000\n",
      "V(2, 0) = 0.590\n",
      "V(2, 1) = 0.656\n",
      "V(2, 2) = 0.729\n",
      "V(2, 3) = 0.656\n"
     ]
    }
   ],
   "source": [
    "for state in gw.states:\n",
    "    print(f'V{state} = {gw.state_values[state]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota que los estados cercanos a (0,3) tienen valores cercanos y +1. Esto se debe a que el estado terminal (0,3) tiene un valor igual a la recompensa recibida en este estado y se propaga a los demas estados, de acuerdo a la política seguida. Note que los estados mas lejanos a (0,3) como por ejemplo (0,0), (1,0) y (2,0) tienes valores bajos pero siguen siendo positivos. Esto es porque la política define una secuencia de acciones desde cualquiera de estos tres estados al estado terminal (0,3). Sin embargo, la recompensa recibida presenta un mayor descuento, pues se deben realizar más pasos para llegar hasta (0,3).\n",
    "\n",
    "* Desde (0,0) hasta (0,3) --> 7 pasos --> $v(s)=\\gamma^7*R=0.4783$\n",
    "* Desde (1,0) hasta (0,3) --> 6 pasos --> $v(s)=\\gamma^6*R=0.5314$\n",
    "* Desde (2,0) hasta (0,3) --> 5 pasos --> $v(s)=\\gamma^5*R=0.5905$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Ahora sí, es momento de implementar los métodos que permiten obtener una política para maximizar las recompensas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value iteration $V^{*}$\n",
    "\n",
    "Este método permite aproximar los valores óptimos de los estados, de tal manera que se pueda encontrar, al final, una política $\\pi^{*}(s)$ que permita maximizar la recompensa recibida por el agente a lo largo del horizonte.\n",
    "\n",
    "$$V^{*}(s)=\\max_{\\pi} \\mathbb{E}\\left[ \\sum_{t=0}^{H} \\gamma^{t} R(s_t,a_t,s_{t+1}) | \\pi, s_0=s \\right]$$\n",
    "\n",
    "Cuando el horizonte es $H=0$, el valor de todos los estado es igual a su valor de inicialización, generalmente cero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import MethodType\n",
    "from gridworld import GridWorld, pygame\n",
    "import copy\n",
    "from colorama import Fore\n",
    "\n",
    "gw = GridWorld(3, 4, [(1,1)], [(1,3)], [(0,3)], 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar valores de los estados\n",
    "gw.state_values = gw.init_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifique los valores de los siguientes estados: (2,0), (0,3), (1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# EDITABLE\n",
    "print(gw.state_values[(2,0)])\n",
    "print(gw.state_values[(0,3)])\n",
    "print(gw.state_values[(1,3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado esperado**\n",
    "```\n",
    "V(2,0) = 0.0\n",
    "V(0,3) = 0.0\n",
    "V(1,3) = 0.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando el horizonte es $H=1$, el valor del estado $s$ corresponde la recompensa recibida para todas las transiciones a $s'$ posibles + el valor del nuevo estado $s'$:\n",
    "$$V_{1}^{*}(s)=\\max_a \\sum_{s'} P(s'|s,a)(R(s,a,s')+\\gamma V_{0}^{*}(s'))$$\n",
    "\n",
    "Pero si en $H=1$ el agente está en un estado terminal, por ejemplo (0,3), el valor de $V_1(0,3)$ se actualizaría así:\n",
    "$$V_{1}^{*}(0,3)=\\max_a \\sum_{s'} P(s'|s,a)(R(s,a,s')$$\n",
    "En este caso se omite el término $V_{0}^{*}(s')$ ya que esta transición no existe al finalizarce el episodio. Por esta razón, el valor de los estados terminales es igual a la recompensa recibida en ellos.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se ajusta el valor de los estados terminales\n",
    "gw.state_values[(0,3)] = 1.0\n",
    "gw.state_values[(1,3)] = -1.0    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si el horizonte es $H=2$, para estados no terminales:\n",
    "$$V_{2}^{*}(s)=\\max_a \\sum_{s'} P(s'|s,a)(R(s,a,s')+\\gamma V_{1}^{*}(s'))$$\n",
    "Por ejemplo, si el agente está en (0,2) y se tiene un descuento $\\gamma=0.90$:\n",
    "\n",
    "* Si pretende ejecutar 'E':\n",
    "    * Si efectivamente se ejecuta 'E':  $ P(s'|s,a)(R(s,a,s')+\\gamma V_{1}^{*}(s')) = 0.8(0.0+\\gamma V_1(0,3))$\n",
    "    * Si, por ruido, se ejecuta 'N':  $ P(s'|s,a)(R(s,a,s')+\\gamma V_{1}^{*}(s')) = 0.1(0.0+\\gamma V_1(0,2))$\n",
    "    * Si, por ruido, se ejecuta 'S':  $ P(s'|s,a)(R(s,a,s')+\\gamma V_{1}^{*}(s')) = 0.1(0.0+\\gamma V_1(1,2))$\n",
    "    * Valor = $0.8(0.9*1.0)+0.1(0.0)+0.1*(0.0)=0.72$\n",
    "    \n",
    "* Si pretende ejecutar 'W':\n",
    "    * Si efectivamente se ejecuta 'W':  $ P(s'|s,a)(R(s,a,s')+\\gamma V_{1}^{*}(s')) = 0.8(0.0+\\gamma V_1(0,1))$\n",
    "    * Si, por ruido, se ejecuta 'N':  $ P(s'|s,a)(R(s,a,s')+\\gamma V_{1}^{*}(s')) = 0.1(0.0+\\gamma V_1(0,2))$\n",
    "    * Si, por ruido, se ejecuta 'S':  $ P(s'|s,a)(R(s,a,s')+\\gamma V_{1}^{*}(s')) = 0.1(0.0+\\gamma V_1(1,2))$\n",
    "    * Valor = $0.8(0.0)+0.1(0.0)+0.1*(0.0)=0.0$\n",
    "\n",
    "* Si pretende ejecutar 'N':\n",
    "    * Si efectivamente se ejecuta 'N':  $ P(s'|s,a)(R(s,a,s')+\\gamma V_{1}^{*}(s')) = 0.8(0.0+\\gamma V_1(0,2))$\n",
    "    * Si, por ruido, se ejecuta 'E':  $ P(s'|s,a)(R(s,a,s')+\\gamma V_{1}^{*}(s')) = 0.1(0.0+\\gamma V_1(0,3))$\n",
    "    * Si, por ruido, se ejecuta 'W':  $ P(s'|s,a)(R(s,a,s')+\\gamma V_{1}^{*}(s')) = 0.1(0.0+\\gamma V_1(0,1))$\n",
    "    * Valor = $0.8(0.0)+0.1(0.9*1.0)+0.1*(0.0)=0.09$\n",
    "    \n",
    "* Si pretende ejecutar 'S':\n",
    "    * Si efectivamente se ejecuta 'S':  $ P(s'|s,a)(R(s,a,s')+\\gamma V_{1}^{*}(s')) = 0.8(0.0+\\gamma V_1(1,2))$\n",
    "    * Si, por ruido, se ejecuta 'E':  $ P(s'|s,a)(R(s,a,s')+\\gamma V_{1}^{*}(s')) = 0.1(0.0+\\gamma V_1(0,3))$\n",
    "    * Si, por ruido, se ejecuta 'W':  $ P(s'|s,a)(R(s,a,s')+\\gamma V_{1}^{*}(s')) = 0.1(0.0+\\gamma V_1(0,1))$\n",
    "    * Valor = $0.8(0.0)+0.1(0.9*1.0)+0.1*(0.0)=0.09$\n",
    "    \n",
    "Por ende, la acción que maximiza el valor del estado (0,2) es 'E' y $V_{2}^{*}(0,2)=0.72$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo\n",
    ">Inicializar el valor de todos los estados como $V_{0}^{*}(s)=0$<br>\n",
    ">Para $k=1, \\cdots, H$:<br>\n",
    ">> Para todos los estados $s$:<br>\n",
    ">>> $\\displaystyle V_{k}^{*}(s)=\\max_a \\sum_{s'} P(s'|s,a)(R(s,a,s')+\\gamma V_{k-1}^{*}(s'))$<br>\n",
    ">>> $\\displaystyle \\pi_{k}^{*}(s)=arg\\max_a \\sum_{s'} P(s'|s,a)(R(s,a,s')+\\gamma V_{k-1}^{*}(s'))$<br>\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementemos el algoritmo de value iteration. Las siguientes funciones o atributos pueden ser útiles:\n",
    "* `gw.get_allowed_actions(state)`: Permite conocer el espacio de acciones.\n",
    "* `gw.real_actions[action]`: Las posibles acciones (reales) que puede tomar el agente.\n",
    "* `gw.action_probabilities[action]`: La probabilidad de que el agente tomé las acciones reales.\n",
    "* `gw.step(state, action, random)`: Permite ejecutar un paso en el gridworld. Con este método se puede conocer el estado siguiente, la recompensa, la accion tomada y la bandera de un estado terminal. Nota: si el parámetro random es Falso, las acciones son determinisiticas (con probabilidad 1, toma la acción dada).\n",
    "* `gw.state_values[state]`: Los valores del estado *state*.\n",
    "* `gw.policy[state]`: La acción dada por la política para el estado *state*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDITABLE\n",
    "\n",
    "def value_iteration(gw, gamma):    \n",
    "    for state in gw.states:       \n",
    "        q = dict.fromkeys(gw.get_allowed_actions(state), 0.0) \n",
    "        \n",
    "        # TO DO: Actualice los valores q para Value iteration\n",
    "        for action in gw.get_allowed_actions(state):\n",
    "            for i, real_action in enumerate(gw.real_actions[action]):\n",
    "                state_prima, reward, _, done = gw.step(state, real_action, random = False)\n",
    "                if done == True:\n",
    "                    q[action] = reward\n",
    "                else:\n",
    "                    q[action] = q[action] + gw.action_probabilities[i]*(reward + gamma*gw.state_values[state_prima])\n",
    "        \n",
    "        \n",
    "        gw.policy[state], gw.state_values[state] =  gw.key_max(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tenga en cuenta:** Si tiene un error en la función 'value_iteration', despues de corregirlo es necesario que vuelva a ejecutar la celda anterior para tener en cuenta el cambio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifique con la siguiente celda que el valor del estado (0,2) es el correcto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7200000000000001\n",
      "E\n"
     ]
    }
   ],
   "source": [
    "# EDITABLE\n",
    "value_iteration(gw, gamma)\n",
    "print(gw.state_values[(0,2)])\n",
    "print(gw.policy[(0,2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado esperado\\*:**\n",
    "```\n",
    "Para H=2, V(0, 2) = 0.7200000000000001\n",
    "Para H=2, acción que maximiza el valor = E\n",
    "```\n",
    "\n",
    "\\* Recuerde reiniciar los valores y ejecutar una única vez el entorno si desea tener un horizonte H=2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora resuelva el gridworld por *value iteration* utilizando la función que usted implementó:\n",
    "* Incialice los estados iniciales\n",
    "* Establezca un $\\gamma=0.90$, un horizonte de 15 iteraciones y el estado inicial en (2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDITABLE\n",
    "\n",
    "# Se define una política inicial cualquiera\n",
    "gw.policy = dict.fromkeys(gw.states, 'N')\n",
    "\n",
    "# Inicializar valores de los estados\n",
    "gw.state_values = gw.init_values()\n",
    "\n",
    "gamma = 0.9\n",
    "H = 15\n",
    "init_state = (0,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualización:**\n",
    "1. Ejecute la siguiente celda y haga *clic* sobre la ventana del gridworld. \n",
    "2. Podra ver como se actualizan los valores de cada estado presionando la tecla **Espacio**. \n",
    "3. Una vez completes las 30 iteraciones, presiona la tecla **Enter** para ver la política\n",
    "4. Puede cerrar el gridworld presionando la tecla **Esc**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw.value_iteration = MethodType(value_iteration, gw)\n",
    "gw.solve_value_iteration(gamma=gamma, horizon=H, init_state=init_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Para un horizonte de 15, los valores de los estados son: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0) 0.6449688734410074\n",
      "(0, 1) 0.744380143233867\n",
      "(0, 2) 0.8477662778685497\n",
      "(0, 3) 1.0\n",
      "(1, 0) 0.5663137205171657\n",
      "(1, 2) 0.5718590329727017\n",
      "(1, 3) -1.0\n",
      "(2, 0) 0.49068161767146456\n",
      "(2, 1) 0.4308405023132072\n",
      "(2, 2) 0.4754706349982549\n",
      "(2, 3) 0.27729534324846455\n"
     ]
    }
   ],
   "source": [
    "# EDITABLE\n",
    "for state in gw.states:\n",
    "    print(state, gw.state_values[state])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado esperado:**\n",
    "```\n",
    "V(0, 0) = 0.645\n",
    "V(0, 1) = 0.744\n",
    "V(0, 2) = 0.848\n",
    "V(0, 3) = 1.000\n",
    "V(1, 0) = 0.566\n",
    "V(1, 2) = 0.572\n",
    "V(1, 3) = -1.000\n",
    "V(2, 0) = 0.491\n",
    "V(2, 1) = 0.431\n",
    "V(2, 2) = 0.475\n",
    "V(2, 3) = 0.277\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La política aprendida es para maximizar la recompensa desde el estado inicial (2,0) es:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acción a tomar en (0, 0) : E\n",
      "Acción a tomar en (0, 1) : E\n",
      "Acción a tomar en (0, 2) : E\n",
      "Acción a tomar en (0, 3) : N\n",
      "Acción a tomar en (1, 0) : N\n",
      "Acción a tomar en (1, 2) : N\n",
      "Acción a tomar en (1, 3) : N\n",
      "Acción a tomar en (2, 0) : N\n",
      "Acción a tomar en (2, 1) : W\n",
      "Acción a tomar en (2, 2) : N\n",
      "Acción a tomar en (2, 3) : W\n"
     ]
    }
   ],
   "source": [
    "# EDITABLE\n",
    "for state in gw.states:\n",
    "    print('Acción a tomar en', state, ':', gw.policy[state])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado esperado:**\n",
    "```\n",
    "Acción a tomar en (0, 0): E\n",
    "Acción a tomar en (0, 1): E\n",
    "Acción a tomar en (0, 2): E\n",
    "Acción a tomar en (0, 3): N\n",
    "Acción a tomar en (1, 0): N\n",
    "Acción a tomar en (1, 2): N\n",
    "Acción a tomar en (1, 3): N\n",
    "Acción a tomar en (2, 0): N\n",
    "Acción a tomar en (2, 1): W\n",
    "Acción a tomar en (2, 2): N\n",
    "Acción a tomar en (2, 3): W\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration\n",
    "\n",
    "Es otro método que busca aproximar los valores óptimos de cada estado basados en la política actual $\\pi(s)$. La politica puede mejorarse con el tiempo, de acuerdo a los valores Q de cada par estado-acción.\n",
    "\n",
    "### Valores Q\n",
    "$Q^{*}(s,a)$ es el valor esperado de la utilidad, si el agente comenzara en el estado $s$, tomando la acción $a$ y comportandose óptimamente en adelante.\n",
    "\n",
    "Los valores Q pueden aproximarse así:\n",
    "$$Q_{k+1}^{*}(s,a)\\leftarrow \\sum_{s'} P(s'|s,a)(R(s,a,s')+\\gamma \\max_{a'}Q_{k}^{*}(s',a'))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import MethodType\n",
    "from gridworld import GridWorld, pygame\n",
    "import copy\n",
    "from colorama import Fore\n",
    "\n",
    "gw = GridWorld(3, 4, [(1,1)], [(1,3)], [(0,3)], 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se inicializan los valores de los estados en 0.0, al igual que los valores Q para todo par estado acción. Además, inicializamos una política que por defecto siempre toma el norte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar valores y política\n",
    "import random\n",
    "gw.state_values = gw.init_values()\n",
    "gw.state_q_values = gw.init_qvalues()\n",
    "gw.policy = dict.fromkeys(gw.states, 'N') \n",
    "gw.updateGrid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifique la cantidad de valores q:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EDITABLE\n",
    "len(gw.state_q_values)*4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado esperado:**\n",
    "\n",
    "```\n",
    "num_q_values = 44\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo\n",
    "\n",
    "> Inicializar $\\pi_{0}(s)$ de forma arbitraria<br>\n",
    "> Inicializar $V_{0}(s)$ y $Q_{0}(s,a)$ en 0.0 para todos los estados y acciones<br>\n",
    "> policy_stable = False<br>\n",
    "> Mientras(not policy_stable):<br>\n",
    ">> 1. Evaluar la política<br>\n",
    ">> Para $k=1,\\cdots,H$:<br>\n",
    ">>> Para todos los estados $s$:<br>\n",
    ">>>> Para todas las acciones $s$ permitidas en $s$:<br>\n",
    ">>>>> $$Q_{k}(s,a)=\\sum_{s'} P(s'|s,\\pi(s))(R(s,a,s')+\\gamma Q_{k-1}(s',\\pi(s')))$$<br>\n",
    ">>>> Valor del estado $s$ corresponde al valor Q de la acción a ejecutar según la política $\\pi_k(s)$ <br>\n",
    ">>>> $V_{k}(s)=Q_{k}(s,\\pi_{k}(s))$ <br>\n",
    ">\n",
    ">> 2. Mejorar la política <br>\n",
    ">> policy_stable = True <br>\n",
    ">> Para todos los estados $s$:<br>\n",
    ">>> best_action = $arg\\max_{a}Q_k(s,)$<br>\n",
    ">>> si *best_action* es diferente a lo que dice $\\pi_{k}(s)$:<br>\n",
    ">>>> $\\pi_{k}(s) = best\\_action$<br>\n",
    ">>>> policy_stable = False<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implemente la función de *policy evaluation* en donde se actualizan los valores $Q(s,a)$ y $V(s)$ de acuerdo a la política:\n",
    "\n",
    "Las siguientes funciones pueden ser útiles:\n",
    "\n",
    "* `gw.get_allowed_actions(state)`: Permite conocer el espacio de acciones.\n",
    "* `gw.real_actions[action]`: Las posibles acciones (reales) que puede tomar el agente.\n",
    "* `gw.action_probabilities[action]`: La probabilidad de que el agente tomé las acciones reales.\n",
    "* `gw.step(state, action, random)`: Permite ejecutar un paso en el gridworld. Con este método se puede conocer el estado siguiente, la recompensa, la accion tomada y la bandera de un estado terminal. Nota: si el parámetro random es Falso, las acciones son determinisiticas (con probabilidad 1, toma la acción dada).\n",
    "* `gw.state_values[state]`: Los valores del estado *state*.\n",
    "* `gw.policy[state]`: La acción dada por la política para el estado *state*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDITABLE\n",
    "def policy_evaluation(gw, gamma):        \n",
    "    g_tmp = {}\n",
    "    for state in gw.states:\n",
    "        q = dict.fromkeys(gw.get_allowed_actions(state), 0.0) \n",
    "        # TO DO: Actualice los valores de los estados con Policy Evaluation\n",
    "        for action in gw.get_allowed_actions(state):\n",
    "            for i, real_action in enumerate(gw.real_actions[action]):\n",
    "                state_prima, reward, _, done = gw.step(state, real_action, random = False)\n",
    "                if done == True:\n",
    "                    q[action] = reward\n",
    "                else:\n",
    "                    q[action] = q[action] + gw.action_probabilities[i]*(reward + gamma*gw.state_q_values[state_prima][gw.policy[state_prima]])\n",
    "        \n",
    "            \n",
    "        #gw.state_values[state] =  gw.max_val(gw.state_q_values[state])\n",
    "        gw.state_q_values[state] = q.copy()\n",
    "        gw.state_values[state] = gw.state_q_values[state][gw.policy[state]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora implemente la función *policy improvement* que permitirá ajustar la política $\\pi(s)$ de acuerdo a la nueva estimación de los valores $Q(s,a)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDITABLE\n",
    "def policy_improvement(gw):\n",
    "    # Bandera de que la política es estable\n",
    "    policy_stable = True\n",
    "    for state in gw.states:\n",
    "        \n",
    "        # TO DO: Actualice la política\n",
    "        best_action, _ =  gw.key_max(gw.state_q_values[state])\n",
    "        \n",
    "        if best_action != gw.policy[state]:\n",
    "            gw.policy[state] = best_action\n",
    "            policy_stable = False\n",
    "        \n",
    "    return policy_stable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora resuelva el gridworld por *policy iteration* utilizando las funciones que usted implementó:\n",
    "* Incialice los estados iniciales\n",
    "* Establezca un $\\gamma=0.99$, un horizonte de 15 iteraciones y el estado inicial en (2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDITABLE\n",
    "gamma = 0.99\n",
    "H = 15\n",
    "init_state = (2,0)\n",
    "\n",
    "gw.state_values = gw.init_values()\n",
    "gw.state_q_values = gw.init_qvalues()\n",
    "gw.policy = dict.fromkeys(gw.states, 'N') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualización:**\n",
    "1. Ejecute la siguiente celda y haga clic sobre la ventana del gridworld. \n",
    "2. Podrá ir viendo como se actualizan los valores de cada estado cada vez que presionas la tecla **Espacio**. \n",
    "3. Tambien puede presionar la tecla Q en la ventana del gridworld para ver los valores Q de cada par (s,a)\n",
    "4. Si tiene un error en la función 'policy_evaluation' o en 'policy_improvement', despues de corregirlo es necesario volver a correr las 5 celdas anteriores para poder volver a lanzar la interfaz del gridworld.\n",
    "5. Una vez completadas las iteraciones, aparecerá un mensaje que dice TESTING. Presiona la tecla **Enter** para ver la política\n",
    "6. Para cerrar el gridworld presiona la tecla **Esc**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw.policy_evaluation = MethodType(policy_evaluation, gw)\n",
    "gw.policy_improvement = MethodType(policy_improvement, gw)\n",
    "gw.solve_policy_iteration(gamma=gamma, horizon=H, init_state=init_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con *policy iteration*, los valores de los estados son: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0) 0.9506129584805684\n",
      "(0, 1) 0.9643218987779846\n",
      "(0, 2) 0.9766720108876323\n",
      "(0, 3) 1.0\n",
      "(1, 0) 0.9387069099493008\n",
      "(1, 2) 0.8898239753176753\n",
      "(1, 3) -1.0\n",
      "(2, 0) 0.9255142363369143\n",
      "(2, 1) 0.9139166154791812\n",
      "(2, 2) 0.9010907849586545\n",
      "(2, 3) 0.7881331049169006\n"
     ]
    }
   ],
   "source": [
    "# EDITABLE\n",
    "for state in gw.states:\n",
    "    print(state, gw.state_values[state])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado esperado:**\n",
    "```\n",
    "V(0, 0) = 0.951\n",
    "V(0, 1) = 0.964\n",
    "V(0, 2) = 0.977\n",
    "V(0, 3) = 1.000\n",
    "V(1, 0) = 0.939\n",
    "V(1, 2) = 0.890\n",
    "V(1, 3) = -1.000\n",
    "V(2, 0) = 0.925\n",
    "V(2, 1) = 0.913\n",
    "V(2, 2) = 0.900\n",
    "V(2, 3) = 0.790\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La política aprendida es para maximizar la recompensa desde el estado inicial (2,0) es:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acción a tomar en (0, 0) : E\n",
      "Acción a tomar en (0, 1) : E\n",
      "Acción a tomar en (0, 2) : E\n",
      "Acción a tomar en (0, 3) : N\n",
      "Acción a tomar en (1, 0) : N\n",
      "Acción a tomar en (1, 2) : W\n",
      "Acción a tomar en (1, 3) : N\n",
      "Acción a tomar en (2, 0) : N\n",
      "Acción a tomar en (2, 1) : W\n",
      "Acción a tomar en (2, 2) : W\n",
      "Acción a tomar en (2, 3) : S\n"
     ]
    }
   ],
   "source": [
    "# EDITABLE\n",
    "for state in gw.states:\n",
    "    print('Acción a tomar en', state, ':', gw.policy[state])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado esperado:**\n",
    "```\n",
    "Acción a tomar en (0, 0): E\n",
    "Acción a tomar en (0, 1): E\n",
    "Acción a tomar en (0, 2): E\n",
    "Acción a tomar en (0, 3): N\n",
    "Acción a tomar en (1, 0): N\n",
    "Acción a tomar en (1, 2): W\n",
    "Acción a tomar en (1, 3): N\n",
    "Acción a tomar en (2, 0): N\n",
    "Acción a tomar en (2, 1): W\n",
    "Acción a tomar en (2, 2): W\n",
    "Acción a tomar en (2, 3): S\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esto termina la implementación de los algoritmos de Policy y Value iteration, puede vovlerlos a ejecutar como desee y variar sus parámetros para ver los efectos que estos tienen."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
