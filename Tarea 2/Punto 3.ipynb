{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "766ae1c1",
   "metadata": {},
   "source": [
    "<br>    \n",
    "<img src=\"https://uniandes.edu.co/sites/default/files/logo-uniandes.png\" width=200 height=\"100\" align=\"left\">\n",
    "<h1 style=\"text-align:center;\">IELE 4922</h1>\n",
    "<h3 style='text-align: right;'> Departamento de Ingeniería Eléctrica y Electrónica &emsp;<br> Facultad de Ingeniería &emsp; <br>\n",
    "     Universidad de los Andes &emsp; <br>\n",
    "     Taller # 2 &emsp; <br>\n",
    "</h3>\n",
    "&emsp;<b>Integrante 1:</b> Mauricio Ricardo Delgado Quintero - 201712801\n",
    "<br>\n",
    "&emsp;<b>Integrante 2:</b> Juan Camilo Pico Garrido - 201731674\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdba8d9",
   "metadata": {},
   "source": [
    "# Punto 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c09de1b",
   "metadata": {},
   "source": [
    "(*The Lazy Adaptable Lion* [<span style=\"color:red\">Puterman, 2014,</span> Ch. 3])\n",
    "\n",
    "Un león requiere comer 6kg de carne por día y puede almacenar en su estomago hasta 30kg. Esto significa que con el estomago lleno, puede sobrevivir 5 días sin comer. Las cebras tienen una biomasa comestible promedio de 164kgm lo suficiente para cubrir las necesidades alimenticias de varios leones.\n",
    "<img src=\"leones.png\" width=400>\n",
    "Si el león caza en grupo, las probabilidades de tener una caza exitosa aumentan con el número de integrantes del grupo. Cada vez que el león sale de caza, esto le representa un consumo energético de 0.5kg. Por observación se tiene que las probabilidades de una caza exitosa son:\n",
    "\n",
    "$$p(1)=0.10+c(0.01) \\quad p(2)=0.27-(10-c)(0.01) \\quad p(3)=0.30$$\n",
    "$$p(4)=0.33 \\quad p(5)=0.33+c(0.01) \\quad p(6)=0.5-(10-c)(0.01)$$\n",
    "\n",
    "Donde $p(n)$ es la probabilidad de cazar a una presa en un grupo de $n$ leones y $c$ es el último número de su código Uniandes.\n",
    "\n",
    "a) Formule este problema como un proceso de decisión de Markov (MDP). Es decir, defina estados, acciones y recompensas. Asuma una única caza por día y que sí la caza es exitosa todos los leones se dividen la carne en partes iguales. Tenga en cuenta que el objetivo del león es maxizimizar su probabilidad de supervivencia sobre $T$ días."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ea10e1",
   "metadata": {},
   "source": [
    "## Solución\n",
    "$$\\text {Variable de estado }  \\\\ X_n: \\text {Cantidad de alimento (en kg) en el estomago de un león después de la n-ésima caza} $$\n",
    "$$\\text {Espacio de estados } \\\\S : \\{0,0.5,1,...,30\\}  $$\n",
    "Por fines prácticos se decide que el espacio de estados estará discretizado en pasos de 0.5 kg\n",
    "$$\\text {Acciones}: \\{0,1,2,3,4,5,6\\}$$\n",
    "Donde 0 significa que el león decide no cazar, 1 que decide cazar individualmente, 2,3,4,5,6 que caza en grupo de tamaño 2,3,4,5 y 6 respectivamente.\n",
    "$$\\text {Recompensas}: $$\n",
    "    $$ r_{t}(s,a)  = \n",
    "    \\begin{cases} \n",
    "    -1, & s=0 \\\\\n",
    "    0, & \\text{d.l.c}\n",
    "    \\end{cases}\n",
    "    $$\n",
    "\n",
    "Ahora bien, la forma de calcular las matrices de probabilidad de transición, es la siguiente:\n",
    "$$ p(i,j|a)= \n",
    "    \\begin{cases}\n",
    "    1, & j=i-12 & i>12 & a=0 \\\\\n",
    "    1, & j=0 & i\\leq12 & a=0 \\\\\n",
    "    1-p(a), & j=i-13 & i>13 & i>0 & a\\neq0 \\\\\n",
    "    1-p(a), & j=0 & i\\leq 13 & i>0 & a\\neq0 \\\\\n",
    "    p(a), & j=60 & i+ \\lfloor \\frac{164\\cdot2}{a}\\rfloor -13 >60 & i>0 & a\\neq0 \\\\\n",
    "    p(a), & j=i+ \\lfloor \\frac{164\\cdot2}{a}\\rfloor -13 & i>0 & a\\neq0 \\\\\n",
    "    1, & i=0 & j=0 & a\\neq0 \\\\\n",
    "    0, & d.l.c\n",
    "    \\end {cases}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "407c14e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingrese el último digito de su código uniandes: 4\n"
     ]
    }
   ],
   "source": [
    "# Importar librerias\n",
    "import numpy as np\n",
    "\n",
    "#Definir el conjunto de estados\n",
    "estados = np.arange(61)\n",
    "estados\n",
    "#Definir el conjunto de acciones \n",
    "acciones = np.arange(7)\n",
    "acciones \n",
    "recompensas = np.zeros(len(estados))\n",
    "recompensas[0] = -1 \n",
    "# Parámetro del último código de alguno de los dos miembros del grupo\n",
    "c = int(input(\"Ingrese el último digito de su código uniandes: \")) \n",
    "\n",
    "#Crear función para calcular las probabilidades fácilmente\n",
    "def prob(a):\n",
    "    if a==1:\n",
    "        resultado = 0.1 + (c*0.01)\n",
    "    if a==2:\n",
    "        resultado = 0.27 - (10-c)*(0.01)\n",
    "    if a==3:\n",
    "        resultado = 0.3\n",
    "    if a==4:\n",
    "        resultado = 0.33\n",
    "    if a==5:\n",
    "        resultado = 0.35 + (c*0.01)\n",
    "    if a==6:\n",
    "        resultado = 0.5 - (10-c)*(0.01)\n",
    "    return resultado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f62266a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear las matrices de transición de probabilidad, una matriz por cada acción\n",
    "P = np.zeros((len(estados),len(estados),len(acciones)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01b8990b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "1\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "2\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "3\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "4\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "5\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "6\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Llenar las matrices de transición de probabilidad\n",
    "for a in acciones:\n",
    "    for i in estados:\n",
    "        for j in estados:\n",
    "            if a==0:\n",
    "                # Consume de lo que tiene almacenado en el estomago\n",
    "                if (j==i-12) & (i>12):\n",
    "                    P[i,j,a] = 1\n",
    "                # Se queda sin reservas\n",
    "                if (j==0) & (i<=12):\n",
    "                    P[i,j,a] = 1\n",
    "            else:\n",
    "                # Caza fallida \n",
    "                if (j==i-13) & (i>0):\n",
    "                    P[i,j,a] = 1-prob(a)\n",
    "                #Caza exitosa \n",
    "                if (j==60) & (i+np.rint((164*2)/a)-13>60) & (i>0):\n",
    "                    P[i,j,a] = prob(a)\n",
    "                #Caza exitosa \n",
    "                if (j==i-13+np.rint((164*2)/a)) & (i>0):\n",
    "                    P[i,j,a] = prob(a)\n",
    "                # Caza fallida y agota las reservas\n",
    "                if (j==0) & (i<=13)  & (i>0):\n",
    "                    P[i,j,a] = 1-prob(a)\n",
    "                # Estado absorbente (está muerto no tiene energía ni siquiera para cazar)\n",
    "                if (j==0) & (i==0):\n",
    "                    P[i,j,a] =1\n",
    "\n",
    "#Comprobar que la matriz sea estocastica\n",
    "for i in acciones:\n",
    "    print(i,P[:,:,i].sum(axis=1),sep=\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b668fd91",
   "metadata": {},
   "source": [
    "## Value iteration "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1615d4",
   "metadata": {},
   "source": [
    "Para hallar una solución óptima al problema del Lazy Adaptable Lion, se empezó por implementar el algoritmo de iteración de valor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "078f9a8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteración: 2  Error: 1.0\n",
      "Iteración: 3  Error: 0.025937084837618057\n",
      "Iteración: 4  Error: 0.020033743101755846\n",
      "Iteración: 5  Error: 0.01542871972644537\n",
      "Iteración: 6  Error: 0.01190331343775751\n",
      "Iteración: 7  Error: 0.00918465076173873\n",
      "Iteración: 8  Error: 0.007086916003957999\n",
      "Iteración: 9  Error: 0.005468294650203209\n",
      "Iteración: 10  Error: 0.004219359501572262\n",
      "Iteración: 11  Error: 0.0032556758082130377\n",
      "Iteración: 12  Error: 0.0025120933554257097\n",
      "Iteración: 13  Error: 0.0019383419597379659\n",
      "Iteración: 14  Error: 0.001495632932894786\n",
      "Iteración: 15  Error: 0.0011540367574057464\n",
      "Iteración: 16  Error: 0.0008904596897754968\n",
      "CPU times: user 291 ms, sys: 5.22 ms, total: 296 ms\n",
      "Wall time: 292 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Crear un vector para guardar la función de valor para cada estado\n",
    "V = np.zeros(len(estados))\n",
    "\n",
    "# Crear un vector para guardar la política óptima\n",
    "Pi = np.zeros(len(estados))\n",
    "\n",
    "# Factor de descuento\n",
    "gamma = 0.9\n",
    "\n",
    "# Contador\n",
    "k = 1\n",
    "\n",
    "error = 1\n",
    "while error > 0.001:\n",
    "    error = 0\n",
    "    \n",
    "    for s in estados:\n",
    "        # Inicializar la funcion de valor de cada acción para el estado s\n",
    "        q = np.zeros(len(acciones))\n",
    "        temp = V[s]*1.0\n",
    "        \n",
    "        # Calcular la función de valor para cada acción\n",
    "        for a in acciones:\n",
    "            if s!=0:\n",
    "                q[a] = sum([P[s, s_prima, a]*(recompensas[s]+gamma*V[s_prima]) for s_prima in estados])\n",
    "            else:\n",
    "                q[a] =  sum([P[s, s_prima, a]*(recompensas[s]) for s_prima in estados])\n",
    "        \n",
    "        # Obtener la función de valor V maximizando q\n",
    "        V[s] = max(q)\n",
    "        # Obtener el argumento (acción) que maximiza q\n",
    "        Pi[s] = np.argmax(q)\n",
    "        \n",
    "        # Determinar si la función de valor ha convergido\n",
    "        error = max(error, abs(temp-V[s]))\n",
    "        \n",
    "    k = k+1\n",
    "    print(\"Iteración:\",k,\" Error:\", error)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003653e7",
   "metadata": {},
   "source": [
    "A continuación se puede observar la función de valor para cada estado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fd2d36e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Estado V\n",
      "0 0.0kg -1.0\n",
      "1 0.5kg -0.5718\n",
      "2 1.0kg -0.5718\n",
      "3 1.5kg -0.5718\n",
      "4 2.0kg -0.5718\n",
      "5 2.5kg -0.5718\n",
      "6 3.0kg -0.5718\n",
      "7 3.5kg -0.5715\n",
      "8 4.0kg -0.5712\n",
      "9 4.5kg -0.5712\n",
      "10 5.0kg -0.565\n",
      "11 5.5kg -0.5603\n",
      "12 6.0kg -0.5603\n",
      "13 6.5kg -0.5146\n",
      "14 7.0kg -0.3445\n",
      "15 7.5kg -0.3445\n",
      "16 8.0kg -0.3445\n",
      "17 8.5kg -0.3445\n",
      "18 9.0kg -0.3445\n",
      "19 9.5kg -0.3445\n",
      "20 10.0kg -0.3443\n",
      "21 10.5kg -0.3442\n",
      "22 11.0kg -0.3442\n",
      "23 11.5kg -0.3411\n",
      "24 12.0kg -0.3387\n",
      "25 12.5kg -0.3387\n",
      "26 13.0kg -0.31\n",
      "27 13.5kg -0.2299\n",
      "28 14.0kg -0.2299\n",
      "29 14.5kg -0.2299\n",
      "30 15.0kg -0.2299\n",
      "31 15.5kg -0.2299\n",
      "32 16.0kg -0.2299\n",
      "33 16.5kg -0.2298\n",
      "34 17.0kg -0.2297\n",
      "35 17.5kg -0.2297\n",
      "36 18.0kg -0.2282\n",
      "37 18.5kg -0.227\n",
      "38 19.0kg -0.227\n",
      "39 19.5kg -0.2069\n",
      "40 20.0kg -0.1721\n",
      "41 20.5kg -0.1721\n",
      "42 21.0kg -0.1721\n",
      "43 21.5kg -0.1721\n",
      "44 22.0kg -0.1721\n",
      "45 22.5kg -0.1721\n",
      "46 23.0kg -0.1721\n",
      "47 23.5kg -0.1721\n",
      "48 24.0kg -0.1721\n",
      "49 24.5kg -0.1713\n",
      "50 25.0kg -0.1707\n",
      "51 25.5kg -0.1707\n",
      "52 26.0kg -0.1549\n",
      "53 26.5kg -0.143\n",
      "54 27.0kg -0.143\n",
      "55 27.5kg -0.143\n",
      "56 28.0kg -0.143\n",
      "57 28.5kg -0.143\n",
      "58 29.0kg -0.143\n",
      "59 29.5kg -0.143\n",
      "60 30.0kg -0.143\n"
     ]
    }
   ],
   "source": [
    "print(\" \", \"Estado\", \"V\")\n",
    "for s in estados:\n",
    "    print(s, \" \", s/2, \"kg\", \" \",round(V[s], 4), sep=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68479683",
   "metadata": {},
   "source": [
    "A continuación se puede observar la acción a tomar en cada estado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23a686ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acción a tomar en 0.0 kg: 0.0\n",
      "Acción a tomar en 0.5 kg: 6.0\n",
      "Acción a tomar en 1.0 kg: 6.0\n",
      "Acción a tomar en 1.5 kg: 6.0\n",
      "Acción a tomar en 2.0 kg: 6.0\n",
      "Acción a tomar en 2.5 kg: 6.0\n",
      "Acción a tomar en 3.0 kg: 6.0\n",
      "Acción a tomar en 3.5 kg: 6.0\n",
      "Acción a tomar en 4.0 kg: 6.0\n",
      "Acción a tomar en 4.5 kg: 6.0\n",
      "Acción a tomar en 5.0 kg: 6.0\n",
      "Acción a tomar en 5.5 kg: 6.0\n",
      "Acción a tomar en 6.0 kg: 6.0\n",
      "Acción a tomar en 6.5 kg: 0.0\n",
      "Acción a tomar en 7.0 kg: 6.0\n",
      "Acción a tomar en 7.5 kg: 6.0\n",
      "Acción a tomar en 8.0 kg: 6.0\n",
      "Acción a tomar en 8.5 kg: 6.0\n",
      "Acción a tomar en 9.0 kg: 6.0\n",
      "Acción a tomar en 9.5 kg: 6.0\n",
      "Acción a tomar en 10.0 kg: 6.0\n",
      "Acción a tomar en 10.5 kg: 6.0\n",
      "Acción a tomar en 11.0 kg: 6.0\n",
      "Acción a tomar en 11.5 kg: 6.0\n",
      "Acción a tomar en 12.0 kg: 6.0\n",
      "Acción a tomar en 12.5 kg: 6.0\n",
      "Acción a tomar en 13.0 kg: 0.0\n",
      "Acción a tomar en 13.5 kg: 6.0\n",
      "Acción a tomar en 14.0 kg: 6.0\n",
      "Acción a tomar en 14.5 kg: 6.0\n",
      "Acción a tomar en 15.0 kg: 6.0\n",
      "Acción a tomar en 15.5 kg: 6.0\n",
      "Acción a tomar en 16.0 kg: 6.0\n",
      "Acción a tomar en 16.5 kg: 6.0\n",
      "Acción a tomar en 17.0 kg: 6.0\n",
      "Acción a tomar en 17.5 kg: 6.0\n",
      "Acción a tomar en 18.0 kg: 6.0\n",
      "Acción a tomar en 18.5 kg: 6.0\n",
      "Acción a tomar en 19.0 kg: 6.0\n",
      "Acción a tomar en 19.5 kg: 0.0\n",
      "Acción a tomar en 20.0 kg: 6.0\n",
      "Acción a tomar en 20.5 kg: 6.0\n",
      "Acción a tomar en 21.0 kg: 6.0\n",
      "Acción a tomar en 21.5 kg: 6.0\n",
      "Acción a tomar en 22.0 kg: 6.0\n",
      "Acción a tomar en 22.5 kg: 6.0\n",
      "Acción a tomar en 23.0 kg: 6.0\n",
      "Acción a tomar en 23.5 kg: 6.0\n",
      "Acción a tomar en 24.0 kg: 6.0\n",
      "Acción a tomar en 24.5 kg: 6.0\n",
      "Acción a tomar en 25.0 kg: 6.0\n",
      "Acción a tomar en 25.5 kg: 6.0\n",
      "Acción a tomar en 26.0 kg: 0.0\n",
      "Acción a tomar en 26.5 kg: 6.0\n",
      "Acción a tomar en 27.0 kg: 6.0\n",
      "Acción a tomar en 27.5 kg: 6.0\n",
      "Acción a tomar en 28.0 kg: 6.0\n",
      "Acción a tomar en 28.5 kg: 6.0\n",
      "Acción a tomar en 29.0 kg: 6.0\n",
      "Acción a tomar en 29.5 kg: 6.0\n",
      "Acción a tomar en 30.0 kg: 6.0\n"
     ]
    }
   ],
   "source": [
    "for s in estados:\n",
    "    print('Acción a tomar en', s/2, \"kg:\", Pi[s])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4d13d8",
   "metadata": {},
   "source": [
    "## Policy iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f030fa",
   "metadata": {},
   "source": [
    "Luego, se implementó el algoritmo de iteración de política. La estructura del algoritmo y si implementación se observan a continuación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906bbad4",
   "metadata": {},
   "source": [
    "### Algoritmo\n",
    "\n",
    "> Inicializar $\\pi_{0}(s)$ de forma arbitraria<br>\n",
    "> Inicializar $V_{0}(s)$ y $Q_{0}(s,a)$ en 0.0 para todos los estados y acciones<br>\n",
    "> policy_stable = False<br>\n",
    "> Mientras(not policy_stable):<br>\n",
    ">> 1. Evaluar la política<br>\n",
    ">> Para $k=1,\\cdots,H$:<br>\n",
    ">>> Para todos los estados $s$:<br>\n",
    ">>>> Para todas las acciones $s$ permitidas en $s$:<br>\n",
    ">>>>> $$Q_{k}(s,a)=\\sum_{s'} P(s'|s,\\pi(s))(R(s,a,s')+\\gamma Q_{k-1}(s',\\pi(s')))$$<br>\n",
    ">>>> Valor del estado $s$ corresponde al valor Q de la acción a ejecutar según la política $\\pi_k(s)$ <br>\n",
    ">>>> $V_{k}(s)=Q_{k}(s,\\pi_{k}(s))$ <br>\n",
    ">\n",
    ">> 2. Mejorar la política <br>\n",
    ">> policy_stable = True <br>\n",
    ">> Para todos los estados $s$:<br>\n",
    ">>> best_action = $arg\\max_{a}Q_k(s,)$<br>\n",
    ">>> si *best_action* es diferente a lo que dice $\\pi_{k}(s)$:<br>\n",
    ">>>> $\\pi_{k}(s) = best\\_action$<br>\n",
    ">>>> policy_stable = False<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab97d9b",
   "metadata": {},
   "source": [
    "Se empezó por crear una función encargada de realizar la evaluación de la política."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65576ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar la política\n",
    "\n",
    "def evaluar_politica():\n",
    "    error = 1\n",
    "    while error > 0.001:\n",
    "        error = 0\n",
    "        for s in estados:\n",
    "            q = np.zeros(len(acciones))\n",
    "            temp = V[s]*1.0\n",
    "\n",
    "            for a in acciones:\n",
    "                if s!=0:\n",
    "                    q[a] = sum([P[s, s_prima, a]*(recompensas[s]+gamma*Q[s_prima, pi_base[s_prima]]) for s_prima in estados])\n",
    "                else:\n",
    "                    q[a] =  sum([P[s, s_prima, a]*(recompensas[s]) for s_prima in estados])\n",
    "            \n",
    "            Q[s,:] = q.copy()\n",
    "\n",
    "            V[s] = Q[s, pi_base[s]]\n",
    "\n",
    "            error = max(error, abs(temp-V[s]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacec0ce",
   "metadata": {},
   "source": [
    "Luego, se inicializa una política arbitraria que en este caso consta de nunca cazar. También se inicializa en 0 la función de valor y la función de pares estado-acción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4595d68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_base = np.zeros(len(estados)).astype(int)\n",
    "gamma = 0.9\n",
    "Q = np.zeros((len(estados), len(acciones)))\n",
    "V = np.zeros(len(estados))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f99e3e5",
   "metadata": {},
   "source": [
    "Finalmente, se implementa todo el algoritmo, mejorando la política seleccionando la acción greedy hasta que la política converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69e111aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fin iteracion 1\n",
      "fin iteracion 2\n",
      "fin iteracion 3\n",
      "CPU times: user 504 ms, sys: 8.47 ms, total: 513 ms\n",
      "Wall time: 505 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "policy_stable = False\n",
    "c = 1\n",
    "while(not policy_stable):\n",
    "    \n",
    "    # Evaluar la política actual\n",
    "    evaluar_politica()\n",
    "    \n",
    "    policy_stable = True\n",
    "\n",
    "    for s in estados:\n",
    "        \n",
    "        # Seleccionar la acción greedy\n",
    "        best_action = np.argmax(Q[s,:])\n",
    "        \n",
    "        # Verificar si la acción greedy es igual a la acción definida por la política actual\n",
    "        if best_action != pi_base[s]:\n",
    "            pi_base[s] = best_action\n",
    "            policy_stable = False\n",
    "    \n",
    "    print(\"fin iteracion\", c)\n",
    "    c = c + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e234fe87",
   "metadata": {},
   "source": [
    "A continuación se puede observar la función de valor para cada estado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56f29327",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Estado V\n",
      "0 0.0kg -1.0\n",
      "1 0.5kg -0.574\n",
      "2 1.0kg -0.574\n",
      "3 1.5kg -0.574\n",
      "4 2.0kg -0.574\n",
      "5 2.5kg -0.574\n",
      "6 3.0kg -0.574\n",
      "7 3.5kg -0.5737\n",
      "8 4.0kg -0.5734\n",
      "9 4.5kg -0.5734\n",
      "10 5.0kg -0.567\n",
      "11 5.5kg -0.5626\n",
      "12 6.0kg -0.5625\n",
      "13 6.5kg -0.5166\n",
      "14 7.0kg -0.3478\n",
      "15 7.5kg -0.3478\n",
      "16 8.0kg -0.3478\n",
      "17 8.5kg -0.3478\n",
      "18 9.0kg -0.3478\n",
      "19 9.5kg -0.3478\n",
      "20 10.0kg -0.3477\n",
      "21 10.5kg -0.3475\n",
      "22 11.0kg -0.3475\n",
      "23 11.5kg -0.3443\n",
      "24 12.0kg -0.3421\n",
      "25 12.5kg -0.3421\n",
      "26 13.0kg -0.3131\n",
      "27 13.5kg -0.2338\n",
      "28 14.0kg -0.2338\n",
      "29 14.5kg -0.2338\n",
      "30 15.0kg -0.2338\n",
      "31 15.5kg -0.2338\n",
      "32 16.0kg -0.2338\n",
      "33 16.5kg -0.2338\n",
      "34 17.0kg -0.2337\n",
      "35 17.5kg -0.2337\n",
      "36 18.0kg -0.2321\n",
      "37 18.5kg -0.2309\n",
      "38 19.0kg -0.2309\n",
      "39 19.5kg -0.2105\n",
      "40 20.0kg -0.1764\n",
      "41 20.5kg -0.1764\n",
      "42 21.0kg -0.1764\n",
      "43 21.5kg -0.1764\n",
      "44 22.0kg -0.1764\n",
      "45 22.5kg -0.1764\n",
      "46 23.0kg -0.1763\n",
      "47 23.5kg -0.1763\n",
      "48 24.0kg -0.1763\n",
      "49 24.5kg -0.1755\n",
      "50 25.0kg -0.1749\n",
      "51 25.5kg -0.1749\n",
      "52 26.0kg -0.1588\n",
      "53 26.5kg -0.1474\n",
      "54 27.0kg -0.1474\n",
      "55 27.5kg -0.1474\n",
      "56 28.0kg -0.1474\n",
      "57 28.5kg -0.1474\n",
      "58 29.0kg -0.1474\n",
      "59 29.5kg -0.1474\n",
      "60 30.0kg -0.1474\n"
     ]
    }
   ],
   "source": [
    "print(\" \", \"Estado\", \"V\")\n",
    "for s in estados:\n",
    "    print(s, \" \", s/2, \"kg\", \" \",round(V[s], 4), sep=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ca76f0",
   "metadata": {},
   "source": [
    "A continuación se puede observar la acción a tomar en cada estado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a783a452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acción a tomar en 0.0 kg: 0\n",
      "Acción a tomar en 0.5 kg: 6\n",
      "Acción a tomar en 1.0 kg: 6\n",
      "Acción a tomar en 1.5 kg: 6\n",
      "Acción a tomar en 2.0 kg: 6\n",
      "Acción a tomar en 2.5 kg: 6\n",
      "Acción a tomar en 3.0 kg: 6\n",
      "Acción a tomar en 3.5 kg: 6\n",
      "Acción a tomar en 4.0 kg: 6\n",
      "Acción a tomar en 4.5 kg: 6\n",
      "Acción a tomar en 5.0 kg: 6\n",
      "Acción a tomar en 5.5 kg: 6\n",
      "Acción a tomar en 6.0 kg: 6\n",
      "Acción a tomar en 6.5 kg: 0\n",
      "Acción a tomar en 7.0 kg: 6\n",
      "Acción a tomar en 7.5 kg: 6\n",
      "Acción a tomar en 8.0 kg: 6\n",
      "Acción a tomar en 8.5 kg: 6\n",
      "Acción a tomar en 9.0 kg: 6\n",
      "Acción a tomar en 9.5 kg: 6\n",
      "Acción a tomar en 10.0 kg: 6\n",
      "Acción a tomar en 10.5 kg: 6\n",
      "Acción a tomar en 11.0 kg: 6\n",
      "Acción a tomar en 11.5 kg: 6\n",
      "Acción a tomar en 12.0 kg: 6\n",
      "Acción a tomar en 12.5 kg: 6\n",
      "Acción a tomar en 13.0 kg: 0\n",
      "Acción a tomar en 13.5 kg: 6\n",
      "Acción a tomar en 14.0 kg: 6\n",
      "Acción a tomar en 14.5 kg: 6\n",
      "Acción a tomar en 15.0 kg: 6\n",
      "Acción a tomar en 15.5 kg: 6\n",
      "Acción a tomar en 16.0 kg: 6\n",
      "Acción a tomar en 16.5 kg: 6\n",
      "Acción a tomar en 17.0 kg: 6\n",
      "Acción a tomar en 17.5 kg: 6\n",
      "Acción a tomar en 18.0 kg: 6\n",
      "Acción a tomar en 18.5 kg: 6\n",
      "Acción a tomar en 19.0 kg: 6\n",
      "Acción a tomar en 19.5 kg: 0\n",
      "Acción a tomar en 20.0 kg: 6\n",
      "Acción a tomar en 20.5 kg: 6\n",
      "Acción a tomar en 21.0 kg: 6\n",
      "Acción a tomar en 21.5 kg: 6\n",
      "Acción a tomar en 22.0 kg: 6\n",
      "Acción a tomar en 22.5 kg: 6\n",
      "Acción a tomar en 23.0 kg: 6\n",
      "Acción a tomar en 23.5 kg: 6\n",
      "Acción a tomar en 24.0 kg: 6\n",
      "Acción a tomar en 24.5 kg: 6\n",
      "Acción a tomar en 25.0 kg: 6\n",
      "Acción a tomar en 25.5 kg: 6\n",
      "Acción a tomar en 26.0 kg: 0\n",
      "Acción a tomar en 26.5 kg: 6\n",
      "Acción a tomar en 27.0 kg: 6\n",
      "Acción a tomar en 27.5 kg: 6\n",
      "Acción a tomar en 28.0 kg: 6\n",
      "Acción a tomar en 28.5 kg: 6\n",
      "Acción a tomar en 29.0 kg: 6\n",
      "Acción a tomar en 29.5 kg: 6\n",
      "Acción a tomar en 30.0 kg: 6\n"
     ]
    }
   ],
   "source": [
    "for s in estados:\n",
    "    print('Acción a tomar en', s/2, \"kg:\", pi_base[s])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1dda3b",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "- Se puede observar que para ambos algorítmos, se obtienen políticas similares donde en la gran mayoría de los estados se determina que la acción óptima a tomar es la de cazar en un grupo de 6 leones. Aunque las dos políticas no son identicas, las funciones de valor $V$ para ambas políticas son consistentes entre si, por lo que podemos concluir que ambas políticas son óptimas y ambos algoritmos son efectivos.\n",
    "\n",
    "- Se pudo observar que el algoritmo de Value Iteration es más eficiente debido a que tuvo un tiempo de ejecución equivalente al 58% del tiempo de ejecución de Policy iteration. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "419.844px",
    "left": "910px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
